{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nn-hw-2",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNLHw44jLyHMFwl4X5tMzWT"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Настя Панасюк"
      ],
      "metadata": {
        "id": "ygPsCOpEhqH7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JUHyOJfhhnAM",
        "outputId": "5c5f567e-4a51-422f-f5fe-c22896f28efa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1L6ivCo7-18i_fu8bGy091MHEqMS2hMXY\n",
            "To: /content/positive.csv\n",
            "100% 26.2M/26.2M [00:00<00:00, 96.4MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=15gosW2-sW7Gc5s5cBKWPbXypJxJMiOud\n",
            "To: /content/negative.csv\n",
            "100% 24.5M/24.5M [00:00<00:00, 112MB/s] \n"
          ]
        }
      ],
      "source": [
        "!gdown https://drive.google.com/uc?id=1L6ivCo7-18i_fu8bGy091MHEqMS2hMXY\n",
        "!gdown https://drive.google.com/uc?id=15gosW2-sW7Gc5s5cBKWPbXypJxJMiOud"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I. CNN на уровне слов: модель берет слова, пропускает их через Embedding слой. По эмбеддингам проходит CNN c фильтрами с разным окном, полученные результаты конкатенируются друг с другом по глубине, по результату конкатенации еще один сверточный слой, далее max pooling over time, на выходе линейный слой + сигмоида, функция потерь BCELoss. (модель аналогична тому, что мы делали на паре по сверткам, но на уровне слов, а не символов)"
      ],
      "metadata": {
        "id": "Chjf3vi2j7g8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import torch.optim as optim\n",
        "!pip install torchmetrics\n",
        "from torchmetrics import F1\n",
        "from torchmetrics.functional import f1, recall\n",
        "\n",
        "import pandas as pd\n",
        "#import numpy as np\n",
        "from string import punctuation\n",
        "from collections import Counter\n",
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "\n",
        "!pip install pymorphy2 \n",
        "\n",
        "from pymorphy2 import MorphAnalyzer\n",
        "\n",
        "tokenizer = RegexpTokenizer(r'[\\w-]+')\n",
        "morph = MorphAnalyzer()"
      ],
      "metadata": {
        "id": "jEGR7kQtlONg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82d9ca5a-17fd-46c8-b64b-82c31ee9409c"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchmetrics\n",
            "  Downloading torchmetrics-0.6.1-py3-none-any.whl (332 kB)\n",
            "\u001b[?25l\r\u001b[K     |█                               | 10 kB 25.7 MB/s eta 0:00:01\r\u001b[K     |██                              | 20 kB 34.1 MB/s eta 0:00:01\r\u001b[K     |███                             | 30 kB 32.7 MB/s eta 0:00:01\r\u001b[K     |████                            | 40 kB 21.1 MB/s eta 0:00:01\r\u001b[K     |█████                           | 51 kB 19.0 MB/s eta 0:00:01\r\u001b[K     |██████                          | 61 kB 14.3 MB/s eta 0:00:01\r\u001b[K     |███████                         | 71 kB 13.6 MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 81 kB 14.5 MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 92 kB 15.9 MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 102 kB 12.0 MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 112 kB 12.0 MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 122 kB 12.0 MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 133 kB 12.0 MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 143 kB 12.0 MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 153 kB 12.0 MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 163 kB 12.0 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 174 kB 12.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 184 kB 12.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 194 kB 12.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 204 kB 12.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 215 kB 12.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 225 kB 12.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 235 kB 12.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 245 kB 12.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 256 kB 12.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 266 kB 12.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 276 kB 12.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 286 kB 12.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 296 kB 12.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 307 kB 12.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 317 kB 12.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 327 kB 12.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 332 kB 12.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (21.3)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (1.19.5)\n",
            "Requirement already satisfied: torch>=1.3.1 in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (1.10.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.3.1->torchmetrics) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->torchmetrics) (3.0.6)\n",
            "Installing collected packages: torchmetrics\n",
            "Successfully installed torchmetrics-0.6.1\n",
            "Requirement already satisfied: pymorphy2 in /usr/local/lib/python3.7/dist-packages (0.9.1)\n",
            "Requirement already satisfied: docopt>=0.6 in /usr/local/lib/python3.7/dist-packages (from pymorphy2) (0.6.2)\n",
            "Requirement already satisfied: pymorphy2-dicts-ru<3.0,>=2.4 in /usr/local/lib/python3.7/dist-packages (from pymorphy2) (2.4.417127.4579844)\n",
            "Requirement already satisfied: dawg-python>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from pymorphy2) (0.7.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "positive = pd.read_csv('positive.csv', sep=';', names=['id', 'tdate', 'tname', 'ttext', 'ttype', 'trep', 'trtw',\n",
        "        'tfav', 'tstcount', 'tfol', 'tfrien', 'listcount'])\n",
        "negative = pd.read_csv('negative.csv', sep=';', names=['id', 'tdate', 'tname', 'ttext', 'ttype', 'trep', 'trtw',\n",
        "        'tfav', 'tstcount', 'tfol', 'tfrien', 'listcount'])\n",
        "corpus = pd.concat([positive, negative])\n",
        "corpus = corpus[['ttext', 'ttype']].drop_duplicates()\n",
        "corpus['ttype'] = (corpus['ttype'] > 0).astype(int)\n",
        "len(corpus)"
      ],
      "metadata": {
        "id": "hP_yveNMkhHo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de441602-4df8-49dc-f6b7-4c43c5087737"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "217440"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pd.set_option('display.max_colwidth', None)\n",
        "corpus.tail(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "Bi0xk8PamPKg",
        "outputId": "f1cb1138-a4cf-4e0f-f3a0-76da2d670dd1"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ttext</th>\n",
              "      <th>ttype</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>111918</th>\n",
              "      <td>Но не каждый хочет что то исправлять:( http://t.co/QNODDQzuZ7</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>111919</th>\n",
              "      <td>скучаю так :-( только @taaannyaaa вправляет мозги, но я все равно скучаю</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>111920</th>\n",
              "      <td>Вот и в школу, в говно это идти уже надо(</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>111921</th>\n",
              "      <td>RT @_Them__: @LisaBeroud Тауриэль, не грусти :( *обнял*</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>111922</th>\n",
              "      <td>Такси везет меня на работу. Раздумываю приплатить, чтобы меня втащили на пятый этаж. Лифта то нет :(</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                       ttext  ttype\n",
              "111918                                         Но не каждый хочет что то исправлять:( http://t.co/QNODDQzuZ7      0\n",
              "111919                              скучаю так :-( только @taaannyaaa вправляет мозги, но я все равно скучаю      0\n",
              "111920                                                             Вот и в школу, в говно это идти уже надо(      0\n",
              "111921                                               RT @_Them__: @LisaBeroud Тауриэль, не грусти :( *обнял*      0\n",
              "111922  Такси везет меня на работу. Раздумываю приплатить, чтобы меня втащили на пятый этаж. Лифта то нет :(      0"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(line):\n",
        "  line = re.sub('RT[^:]+:', 'USER', line)\n",
        "  line = re.sub('https?[^ ]+', 'LINK', line)\n",
        "  tokens = tokenizer.tokenize(line)\n",
        "  tokens = [token.lower() for token in tokens if re.search('[A-zА-яЁё]', token)]\n",
        "  preprocessed_line = ' '.join(tokens)\n",
        "  return preprocessed_line \n",
        "\n",
        "\n",
        "corpus['text'] = corpus['ttext'].apply(lambda x: tokenize(x))\n",
        "corpus['tokens'] = corpus['text'].apply(lambda x: x.split(' '))"
      ],
      "metadata": {
        "id": "uZ-SLFhKS4qW"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus.head(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 327
        },
        "id": "ILexEn3TXXC5",
        "outputId": "7340339c-787b-4ec2-d662-bcb2ebb33cd9"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ttext</th>\n",
              "      <th>ttype</th>\n",
              "      <th>text</th>\n",
              "      <th>tokens</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>@first_timee хоть я и школота, но поверь, у нас то же самое :D общество профилирующий предмет типа)</td>\n",
              "      <td>1</td>\n",
              "      <td>first_timee хоть я и школота но поверь у нас то же самое d общество профилирующий предмет типа</td>\n",
              "      <td>[first_timee, хоть, я, и, школота, но, поверь, у, нас, то, же, самое, d, общество, профилирующий, предмет, типа]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Да, все-таки он немного похож на него. Но мой мальчик все равно лучше:D</td>\n",
              "      <td>1</td>\n",
              "      <td>да все-таки он немного похож на него но мой мальчик все равно лучше d</td>\n",
              "      <td>[да, все-таки, он, немного, похож, на, него, но, мой, мальчик, все, равно, лучше, d]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>RT @KatiaCheh: Ну ты идиотка) я испугалась за тебя!!!</td>\n",
              "      <td>1</td>\n",
              "      <td>user ну ты идиотка я испугалась за тебя</td>\n",
              "      <td>[user, ну, ты, идиотка, я, испугалась, за, тебя]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>RT @digger2912: \"Кто то в углу сидит и погибает от голода, а мы ещё 2 порции взяли, хотя уже и так жрать не хотим\" :DD http://t.co/GqG6iuE2…</td>\n",
              "      <td>1</td>\n",
              "      <td>user кто то в углу сидит и погибает от голода а мы ещё порции взяли хотя уже и так жрать не хотим dd link</td>\n",
              "      <td>[user, кто, то, в, углу, сидит, и, погибает, от, голода, а, мы, ещё, порции, взяли, хотя, уже, и, так, жрать, не, хотим, dd, link]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>@irina_dyshkant Вот что значит страшилка :D\\nНо блин,посмотрев все части,у тебя создастся ощущение,что авторы курили что-то :D</td>\n",
              "      <td>1</td>\n",
              "      <td>irina_dyshkant вот что значит страшилка d но блин посмотрев все части у тебя создастся ощущение что авторы курили что-то d</td>\n",
              "      <td>[irina_dyshkant, вот, что, значит, страшилка, d, но, блин, посмотрев, все, части, у, тебя, создастся, ощущение, что, авторы, курили, что-то, d]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                                          ttext  ...                                                                                                                                           tokens\n",
              "0                                           @first_timee хоть я и школота, но поверь, у нас то же самое :D общество профилирующий предмет типа)  ...                                 [first_timee, хоть, я, и, школота, но, поверь, у, нас, то, же, самое, d, общество, профилирующий, предмет, типа]\n",
              "1                                                                       Да, все-таки он немного похож на него. Но мой мальчик все равно лучше:D  ...                                                             [да, все-таки, он, немного, похож, на, него, но, мой, мальчик, все, равно, лучше, d]\n",
              "2                                                                                         RT @KatiaCheh: Ну ты идиотка) я испугалась за тебя!!!  ...                                                                                                 [user, ну, ты, идиотка, я, испугалась, за, тебя]\n",
              "3  RT @digger2912: \"Кто то в углу сидит и погибает от голода, а мы ещё 2 порции взяли, хотя уже и так жрать не хотим\" :DD http://t.co/GqG6iuE2…  ...               [user, кто, то, в, углу, сидит, и, погибает, от, голода, а, мы, ещё, порции, взяли, хотя, уже, и, так, жрать, не, хотим, dd, link]\n",
              "4                @irina_dyshkant Вот что значит страшилка :D\\nНо блин,посмотрев все части,у тебя создастся ощущение,что авторы курили что-то :D  ...  [irina_dyshkant, вот, что, значит, страшилка, d, но, блин, посмотрев, все, части, у, тебя, создастся, ощущение, что, авторы, курили, что-то, d]\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus_toks = []\n",
        "for sublist in corpus['text']:\n",
        "    sublist = sublist.split(' ')\n",
        "    for item in sublist:\n",
        "        corpus_toks.append(item)\n",
        "\n",
        "corpus_toks = Counter(corpus_toks)\n",
        "\n",
        "word2id = {'PAD':0}\n",
        "c = 0\n",
        "for k, length in corpus_toks.most_common():\n",
        "  c += 1\n",
        "  if length > 3:\n",
        "    word2id[k] = c\n",
        "\n",
        "len(word2id)"
      ],
      "metadata": {
        "id": "ajaJzoNRcVCV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3b83909-9fbe-422f-a641-fc8428500105"
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "42629"
            ]
          },
          "metadata": {},
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "DEVICE"
      ],
      "metadata": {
        "id": "WU0V8FfTlFd7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7f68282-bb1e-458f-d562-e71174b9fbaf"
      },
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {},
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TweetsDataset(Dataset):\n",
        "\n",
        "    def __init__(self, dataset, word2id, DEVICE):\n",
        "        self.dataset = dataset['tokens'].values\n",
        "        self.word2id = word2id\n",
        "        self.length = dataset.shape[0]\n",
        "        self.target = dataset['ttype'].values\n",
        "        self.device = DEVICE\n",
        "\n",
        "    def __len__(self): #это обязательный метод, он должен уметь считать длину датасета\n",
        "        return self.length\n",
        "\n",
        "    def __getitem__(self, index): #еще один обязательный метод. По индексу возвращает элемент выборки\n",
        "        tokens = self.dataset[index] # токенизируем\n",
        "        ids = torch.LongTensor([self.word2id[token] for token in tokens if token in self.word2id])\n",
        "        y = [self.target[index]]\n",
        "        return ids, y\n",
        "\n",
        "    def collate_fn(self, batch): #этот метод можно реализовывать и отдельно,\n",
        "    # он понадобится для DataLoader во время итерации по батчам\n",
        "      ids, y = list(zip(*batch))\n",
        "      padded_ids = pad_sequence(ids, batch_first=True).to(self.device)\n",
        "      #мы хотим применять BCELoss, он будет брать на вход predicted размера batch_size x 1 (так как для каждого семпла модель будет отдавать одно число), target размера batch_size x 1\n",
        "      y = torch.Tensor(y).to(self.device) # tuple ([1], [0], [1])  -> Tensor [[1.], [0.], [1.]] \n",
        "      return padded_ids, y"
      ],
      "metadata": {
        "id": "FSpTTqg0U3ju"
      },
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data, val_data = train_test_split(corpus[['tokens', 'ttype']], test_size=0.2, stratify=corpus['ttype'], random_state=42)\n",
        "\n",
        "train_dataset = TweetsDataset(train_data, word2id, DEVICE)\n",
        "train_sampler = RandomSampler(train_dataset)\n",
        "train_iterator = DataLoader(train_dataset, collate_fn=train_dataset.collate_fn, sampler=train_sampler, batch_size=1024)\n",
        "\n",
        "val_dataset = TweetsDataset(val_data, word2id, DEVICE)\n",
        "val_sampler = SequentialSampler(val_dataset)\n",
        "val_iterator = DataLoader(val_dataset, collate_fn=train_dataset.collate_fn, sampler=val_sampler, batch_size=1024)"
      ],
      "metadata": {
        "id": "_khajEkQlk19"
      },
      "execution_count": 162,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### cnn"
      ],
      "metadata": {
        "id": "1Y1kGIo5pUL4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### model"
      ],
      "metadata": {
        "id": "fH_6xSoO0JrY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN(nn.Module):\n",
        "    \n",
        "    def __init__(self, vocab_size, embedding_dim, weights=None):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        if weights is not None:\n",
        "            self.embedding.from_pretrained(torch.tensor(weights), freeze=True)\n",
        "        self.bigrams = nn.Conv1d(in_channels=embedding_dim, out_channels=100, kernel_size=2, padding='same')\n",
        "        self.trigrams = nn.Conv1d(in_channels=embedding_dim, out_channels=80, kernel_size=3, padding='same')\n",
        "        self.concated = nn.Conv1d(in_channels=180, out_channels=180, kernel_size=2, padding='same')\n",
        "        self.pooling = nn.MaxPool1d(kernel_size=2, stride=2)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.hidden = nn.Linear(in_features=180, out_features=1)\n",
        "       # self.dropout = nn.Dropout(p=0.5)\n",
        "        self.out = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, word):\n",
        "        #batch_size x seq_len\n",
        "        embedded = self.embedding(word)\n",
        "        #batch_size x seq_len x embedding_dim\n",
        "        embedded = embedded.transpose(1,2)\n",
        "        #batch_size x embedding_dim x seq_len\n",
        "        feature_map_bigrams = self.relu(self.bigrams(embedded))\n",
        "        #batch_size x filter_count2 x seq_len* \n",
        "        feature_map_trigrams = self.relu(self.trigrams(embedded))\n",
        "        \n",
        "        # batch_size x filter_count3\n",
        "        concat = torch.cat((feature_map_bigrams, feature_map_trigrams), 1)\n",
        "        concat = self.pooling(self.relu(self.concated(concat)))\n",
        "        pooling = concat.max(2)[0] \n",
        "        # batch _size x (filter_count2 + filter_count3)\n",
        "        logits = self.hidden(pooling) \n",
        "        logits = self.out(logits)      \n",
        "        return logits"
      ],
      "metadata": {
        "id": "oxOqhcz1npb_"
      },
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, iterator, optimizer, criterion):\n",
        "    print('Training...')\n",
        "    epoch_loss = 0 # для подсчета среднего лосса на всех батчах\n",
        "    model.train()  # ставим модель в обучение, явно указываем, что сейчас надо будет хранить градиенты у всех весов\n",
        "\n",
        "    epoch_loss = 0\n",
        "    epoch_metric = 0\n",
        "\n",
        "    for i, (texts, ys) in enumerate(iterator): #итерируемся по батчам\n",
        "        optimizer.zero_grad()  #обнуляем градиенты\n",
        "        preds_proba = model(texts) #прогоняем данные через модель\n",
        "        loss = criterion(preds_proba, ys) #считаем значение функции потерь  \n",
        "        loss.backward() #считаем градиенты  \n",
        "        optimizer.step() #обновляем веса \n",
        "        epoch_loss += loss.item() #сохраняем значение функции потерь\n",
        "        batch_metric = f1(preds_proba.round().long(), ys.long(), ignore_index=0)\n",
        "        epoch_metric += batch_metric\n",
        "\n",
        "        if not (i + 1) % int(len(iterator)/5):\n",
        "              print(f'Val loss: {epoch_loss/i}, Val f1: {epoch_metric/i}')\n",
        "        \n",
        "    return epoch_metric / len(iterator), epoch_loss / len(iterator) # возвращаем среднее значение по всей выборке\n",
        "\n",
        "def evaluate(model, iterator, criterion):\n",
        "    print(\"\\nValidating...\")\n",
        "    epoch_loss = 0\n",
        "    epoch_metric = 0\n",
        "    model.eval() \n",
        "    with torch.no_grad():\n",
        "        for i, (texts, ys) in enumerate(iterator):   \n",
        "            predictions = model(texts)  # делаем предсказания на тесте\n",
        "            loss = criterion(predictions, ys)   # считаем значения функции ошибки для статистики  \n",
        "            epoch_loss += loss.item()\n",
        "            batch_metric = f1(predictions.round().long(), ys.long(), ignore_index=0)\n",
        "            epoch_metric += batch_metric\n",
        "            if not (i + 1) % int(len(iterator)/5):\n",
        "              print(f'Val loss: {epoch_loss/i}, Val f1: {epoch_metric/i}')\n",
        "  \n",
        "        \n",
        "    return epoch_metric / len(iterator), epoch_loss / len(iterator) # возвращаем средний лосс по батчам\n",
        "\n",
        "model = CNN(len(word2id), 5)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = nn.BCELoss() # Binary Cross Entropy\n",
        "\n",
        "# веса модели и значения лосса храним там же, где и все остальные тензоры\n",
        "model = model.to(DEVICE)\n",
        "criterion = criterion.to(DEVICE)\n",
        "\n",
        "losses = []\n",
        "losses_eval = []\n",
        "\n",
        "for i in range(5): # работаю в колабе, очень плохой интернет, поэтому для теста пока так\n",
        "    print(f'\\nstarting Epoch {i}')\n",
        "    epoch_loss = train(model, train_iterator, optimizer, criterion)\n",
        "    losses.append(epoch_loss)\n",
        "\n",
        "    epoch_loss_on_test = evaluate(model, val_iterator, criterion)\n",
        "    losses_eval.append(epoch_loss_on_test)"
      ],
      "metadata": {
        "id": "R6brdJOGyFj4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1370f648-13ee-4040-d852-103f2ca09dac"
      },
      "execution_count": 163,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "starting Epoch 0\n",
            "Training...\n",
            "Val loss: 0.7013405109896804, Val f1: 0.5279585123062134\n",
            "Val loss: 0.6776460747220623, Val f1: 0.56022709608078\n",
            "Val loss: 0.6643653871989487, Val f1: 0.5701403617858887\n",
            "Val loss: 0.6561833200631318, Val f1: 0.5747734308242798\n",
            "Val loss: 0.649901512459185, Val f1: 0.5802148580551147\n",
            "\n",
            "Validating...\n",
            "Val loss: 0.706622838973999, Val f1: 0.7032313942909241\n",
            "Val loss: 0.6603137175242106, Val f1: 0.6505408883094788\n",
            "Val loss: 0.644349261470463, Val f1: 0.6373050212860107\n",
            "Val loss: 0.6375631074751577, Val f1: 0.6308582425117493\n",
            "Val loss: 0.6334672952309633, Val f1: 0.6269897222518921\n",
            "\n",
            "starting Epoch 1\n",
            "Training...\n",
            "Val loss: 0.6320449583458178, Val f1: 0.6430345177650452\n",
            "Val loss: 0.6206097923108002, Val f1: 0.6364155411720276\n",
            "Val loss: 0.6173112492750187, Val f1: 0.6342986226081848\n",
            "Val loss: 0.6139104057241369, Val f1: 0.6343365907669067\n",
            "Val loss: 0.6110879905830474, Val f1: 0.63584303855896\n",
            "\n",
            "Validating...\n",
            "Val loss: 0.6836206998143878, Val f1: 0.7307418584823608\n",
            "Val loss: 0.638587995370229, Val f1: 0.6758086085319519\n",
            "Val loss: 0.6229797342549199, Val f1: 0.6594889163970947\n",
            "Val loss: 0.6161253606119463, Val f1: 0.6525847911834717\n",
            "Val loss: 0.6124095656932929, Val f1: 0.6487221121788025\n",
            "\n",
            "starting Epoch 2\n",
            "Training...\n",
            "Val loss: 0.6095853773030367, Val f1: 0.6726352572441101\n",
            "Val loss: 0.5975626208888951, Val f1: 0.6710373163223267\n",
            "Val loss: 0.5919446384552682, Val f1: 0.6689427495002747\n",
            "Val loss: 0.5889808495839437, Val f1: 0.668606698513031\n",
            "Val loss: 0.5874049638855386, Val f1: 0.6688806414604187\n",
            "\n",
            "Validating...\n",
            "Val loss: 0.6695594021252224, Val f1: 0.7186242341995239\n",
            "Val loss: 0.6236666719118754, Val f1: 0.6715906262397766\n",
            "Val loss: 0.6081690451373225, Val f1: 0.6570081114768982\n",
            "Val loss: 0.6010285269829535, Val f1: 0.6510270833969116\n",
            "Val loss: 0.5977004919296656, Val f1: 0.6488457322120667\n",
            "\n",
            "starting Epoch 3\n",
            "Training...\n",
            "Val loss: 0.581534353169528, Val f1: 0.7066118717193604\n",
            "Val loss: 0.5714849331485692, Val f1: 0.6969199180603027\n",
            "Val loss: 0.5681608431410081, Val f1: 0.6970601081848145\n",
            "Val loss: 0.5657245340170683, Val f1: 0.6961906552314758\n",
            "Val loss: 0.5628380701386717, Val f1: 0.6971919536590576\n",
            "\n",
            "Validating...\n",
            "Val loss: 0.6436403478894915, Val f1: 0.7984619736671448\n",
            "Val loss: 0.599949852625529, Val f1: 0.7444223165512085\n",
            "Val loss: 0.5866784220156462, Val f1: 0.7267738580703735\n",
            "Val loss: 0.5789113602330608, Val f1: 0.7186716794967651\n",
            "Val loss: 0.5759597435975686, Val f1: 0.7145429253578186\n",
            "\n",
            "starting Epoch 4\n",
            "Training...\n",
            "Val loss: 0.5602639353636539, Val f1: 0.7302273511886597\n",
            "Val loss: 0.5513198731550529, Val f1: 0.7183153629302979\n",
            "Val loss: 0.5473325807269257, Val f1: 0.7161587476730347\n",
            "Val loss: 0.5443162573708429, Val f1: 0.7161952257156372\n",
            "Val loss: 0.5430640391344149, Val f1: 0.7166171073913574\n",
            "\n",
            "Validating...\n",
            "Val loss: 0.6337633899280003, Val f1: 0.8315642476081848\n",
            "Val loss: 0.5926810224850972, Val f1: 0.7697846293449402\n",
            "Val loss: 0.5810116218483966, Val f1: 0.7523367404937744\n",
            "Val loss: 0.5726624977204108, Val f1: 0.7456326484680176\n",
            "Val loss: 0.5694104509475904, Val f1: 0.7414519190788269\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "lss = [los[1] for los in losses]\n",
        "evs = [los[1] for los in losses_eval]\n",
        "plt.plot(lss)\n",
        "plt.plot(evs)\n",
        "plt.title('BCE loss value')\n",
        "plt.ylabel('BCE loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper right')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xDvYaTdS1Nxf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "471c46a0-f61b-4e00-98c7-49df7f966eb1"
      },
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### fasttext"
      ],
      "metadata": {
        "id": "sVIjr_REQ2YJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import FastText\n",
        "fasttext = FastText(corpus['text'].tolist(), size=100, window=5, min_count=3)"
      ],
      "metadata": {
        "id": "0q4d5svIQ1ye"
      },
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "weights = np.zeros((len(word2id), 100))\n",
        "count = 0\n",
        "for word, i in word2id.items():\n",
        "    if word == 'PAD':\n",
        "        continue   \n",
        "    try:\n",
        "        weights[i] = fasttext.wv[word]    \n",
        "    except KeyError:\n",
        "      count += 1\n",
        "      # oov словам сопоставляем случайный вектор\n",
        "      weights[i] = np.random.normal(0,0.1,100)"
      ],
      "metadata": {
        "id": "NhGGCA7qyFne"
      },
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = CNN(len(word2id), 100, weights)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = nn.BCELoss() # Binary Cross Entropy\n",
        "\n",
        "# веса модели и значения лосса храним там же, где и все остальные тензоры\n",
        "model = model.to(DEVICE)\n",
        "criterion = criterion.to(DEVICE)\n",
        "\n",
        "losses_ft = []\n",
        "losses_eval_ft = []\n",
        "\n",
        "for i in range(5):\n",
        "    print(f'\\nstarting Epoch {i}')\n",
        "    epoch_loss = train(model, train_iterator, optimizer, criterion)\n",
        "    losses_ft.append(epoch_loss)\n",
        "\n",
        "    epoch_loss_on_test = evaluate(model, val_iterator, criterion)\n",
        "    losses_eval_ft.append(epoch_loss_on_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gW7AeYj4VOnH",
        "outputId": "b312a95c-fbee-4b7c-d6ae-3296d10bc0f8"
      },
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "starting Epoch 0\n",
            "Training...\n",
            "Val loss: 0.6666884928038626, Val f1: 0.5711579918861389\n",
            "Val loss: 0.6296297987895225, Val f1: 0.6095152497291565\n",
            "Val loss: 0.6120221414188347, Val f1: 0.6298003792762756\n",
            "Val loss: 0.5980467279752095, Val f1: 0.6463456749916077\n",
            "Val loss: 0.5891324546210159, Val f1: 0.65656977891922\n",
            "\n",
            "Validating...\n",
            "Val loss: 0.6107084155082703, Val f1: 0.8317893743515015\n",
            "Val loss: 0.571240262190501, Val f1: 0.7689230442047119\n",
            "Val loss: 0.5600466831870701, Val f1: 0.7475221753120422\n",
            "Val loss: 0.5548958797608653, Val f1: 0.739115297794342\n",
            "Val loss: 0.5528633365264306, Val f1: 0.7329834699630737\n",
            "\n",
            "starting Epoch 1\n",
            "Training...\n",
            "Val loss: 0.5357935961448785, Val f1: 0.7541112303733826\n",
            "Val loss: 0.525522429996462, Val f1: 0.7415659427642822\n",
            "Val loss: 0.5173265054674432, Val f1: 0.7425519824028015\n",
            "Val loss: 0.5120848066276974, Val f1: 0.7446827292442322\n",
            "Val loss: 0.5091721078700567, Val f1: 0.7455108761787415\n",
            "\n",
            "Validating...\n",
            "Val loss: 0.5711358317307064, Val f1: 0.8523087501525879\n",
            "Val loss: 0.5334984640280406, Val f1: 0.792547345161438\n",
            "Val loss: 0.5234037261942158, Val f1: 0.7719455361366272\n",
            "Val loss: 0.5183050757454287, Val f1: 0.7647407650947571\n",
            "Val loss: 0.5172718877975757, Val f1: 0.7593652009963989\n",
            "\n",
            "starting Epoch 2\n",
            "Training...\n",
            "Val loss: 0.4751873467907761, Val f1: 0.7968257069587708\n",
            "Val loss: 0.4663193194723841, Val f1: 0.788514256477356\n",
            "Val loss: 0.46278857742205703, Val f1: 0.7862101793289185\n",
            "Val loss: 0.46071596984510066, Val f1: 0.7849704623222351\n",
            "Val loss: 0.45787824840235286, Val f1: 0.7845177054405212\n",
            "\n",
            "Validating...\n",
            "Val loss: 0.5509269194943565, Val f1: 0.8797382116317749\n",
            "Val loss: 0.5150566915671031, Val f1: 0.8183183670043945\n",
            "Val loss: 0.5071315661720608, Val f1: 0.7968345284461975\n",
            "Val loss: 0.501214676326321, Val f1: 0.7890018224716187\n",
            "Val loss: 0.5015495220820109, Val f1: 0.7832329869270325\n",
            "\n",
            "starting Epoch 3\n",
            "Training...\n",
            "Val loss: 0.4243351970658158, Val f1: 0.8355108499526978\n",
            "Val loss: 0.4152398185053868, Val f1: 0.8252624273300171\n",
            "Val loss: 0.4140788543932509, Val f1: 0.8193437457084656\n",
            "Val loss: 0.41295259043022436, Val f1: 0.8172180652618408\n",
            "Val loss: 0.4127458531475631, Val f1: 0.8156793117523193\n",
            "\n",
            "Validating...\n",
            "Val loss: 0.5529172973973411, Val f1: 0.8812258839607239\n",
            "Val loss: 0.5157931963602702, Val f1: 0.820602297782898\n",
            "Val loss: 0.5065488426581674, Val f1: 0.799328088760376\n",
            "Val loss: 0.500519048783087, Val f1: 0.7906374931335449\n",
            "Val loss: 0.5009038410125635, Val f1: 0.7849997282028198\n",
            "\n",
            "starting Epoch 4\n",
            "Training...\n",
            "Val loss: 0.3757493929429488, Val f1: 0.8667861819267273\n",
            "Val loss: 0.3709241405351838, Val f1: 0.8517219424247742\n",
            "Val loss: 0.37242872644178937, Val f1: 0.8449975252151489\n",
            "Val loss: 0.3715041142922861, Val f1: 0.8426687717437744\n",
            "Val loss: 0.3717280899631907, Val f1: 0.8406394720077515\n",
            "\n",
            "Validating...\n",
            "Val loss: 0.5619447784764426, Val f1: 0.8757966160774231\n",
            "Val loss: 0.5234525243441264, Val f1: 0.8150655627250671\n",
            "Val loss: 0.5132660010586614, Val f1: 0.7942242622375488\n",
            "Val loss: 0.5072605763712237, Val f1: 0.7859136462211609\n",
            "Val loss: 0.5084845698796786, Val f1: 0.7794851660728455\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lss = [los[1] for los in losses_ft]\n",
        "evs = [los[1] for los in losses_eval_ft]\n",
        "plt.plot(lss)\n",
        "plt.plot(evs)\n",
        "plt.title('BCE loss value')\n",
        "plt.ylabel('BCE loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper right')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "02j-NmA4W4o6",
        "outputId": "bdebc96f-25aa-4860-91cb-f838f2565d6e"
      },
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "II. combine word2id + symbol2id"
      ],
      "metadata": {
        "id": "JL28Yra4Wcdq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = Counter()\n",
        "\n",
        "for text in corpus['text']:\n",
        "    for symbol in text:\n",
        "        vocab.update(list(symbol))\n",
        "print('всего уникальных символов:', len(vocab))\n",
        "\n",
        "filtered_vocab = set()\n",
        "\n",
        "for symbol in vocab:\n",
        "    if vocab[symbol] > 5:\n",
        "        filtered_vocab.add(symbol)\n",
        "print('уникальных символов, втретившихся больше 5 раз:', len(filtered_vocab))\n",
        "\n",
        "symbol2id = {'PAD':0}\n",
        "\n",
        "for symbol in filtered_vocab:\n",
        "    symbol2id[symbol] = len(symbol2id)\n",
        "\n",
        "id2symbol = {i:symbol for symbol, i in symbol2id.items()}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3MvwQ-iTWgJm",
        "outputId": "5aa89227-c1e6-4982-ed05-2f499d6633ef"
      },
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "всего уникальных символов: 127\n",
            "уникальных символов, втретившихся больше 5 раз: 90\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CombineDataset(Dataset):\n",
        "\n",
        "    def __init__(self, dataset, word2id, symbol2id, DEVICE):\n",
        "        self.dataset = dataset['tokens'].values\n",
        "        self.text = dataset['text'].values\n",
        "        self.word2id = word2id\n",
        "        self.symbol2id = symbol2id\n",
        "        self.length = dataset.shape[0]\n",
        "        self.target = dataset['ttype'].values\n",
        "        self.device = DEVICE\n",
        "\n",
        "    def __len__(self): #это обязательный метод, он должен уметь считать длину датасета\n",
        "        return self.length\n",
        "\n",
        "    def __getitem__(self, index): #еще один обязательный метод. По индексу возвращает элемент выборки\n",
        "        tokens = self.dataset[index] # токенизируем\n",
        "        symbols = list(self.text)\n",
        "        ids = torch.LongTensor([self.word2id[token] for token in tokens if token in self.word2id])\n",
        "        symbol_ids = torch.LongTensor([self.symbol2id[sym] for sym in symbols if sym in self.symbol2id])\n",
        "        y = [self.target[index]]\n",
        "        return ids, symbol_ids, y\n",
        "\n",
        "    def collate_fn(self, batch): #этот метод можно реализовывать и отдельно,\n",
        "    # он понадобится для DataLoader во время итерации по батчам\n",
        "      ids, symbol_ids, y = list(zip(*batch))\n",
        "      padded_ids = pad_sequence(ids, batch_first=True).to(self.device)\n",
        "      #мы хотим применять BCELoss, он будет брать на вход predicted размера batch_size x 1 (так как для каждого семпла модель будет отдавать одно число), target размера batch_size x 1\n",
        "      padded_sym_ids = pad_sequence(symbol_ids, batch_first=True).to(self.device)\n",
        "      y = torch.Tensor(y).to(self.device) # tuple ([1], [0], [1])  -> Tensor [[1.], [0.], [1.]] \n",
        "      return padded_ids, padded_sym_ids, y"
      ],
      "metadata": {
        "id": "3CazL9eheANx"
      },
      "execution_count": 153,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CombCNN(nn.Module):\n",
        "    def __init__(self, words_vocab, sym_vocab, embedding_dim, weights=None):\n",
        "        super().__init__()\n",
        "        self.word_embedding = nn.Embedding(words_vocab, embedding_dim)\n",
        "        self.sym_embedding = nn.Embedding(sym_vocab, embedding_dim)\n",
        "        if weights is not None:\n",
        "            self.word_embedding.from_pretrained(torch.tensor(weights), freeze=True)\n",
        "        self.bigrams = nn.Conv1d(in_channels=embedding_dim, out_channels=100, kernel_size=2, padding='same')\n",
        "        self.trigrams = nn.Conv1d(in_channels=embedding_dim, out_channels=80, kernel_size=3, padding='same')\n",
        "        self.concated = nn.Conv1d(in_channels=180, out_channels=180, kernel_size=2, padding='same')\n",
        "        self.pooling = nn.MaxPool1d(kernel_size=2, stride=2)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.hidden = nn.Linear(in_features=180, out_features=1)\n",
        "        self.out = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, word, sym):\n",
        "        #batch_size x seq_len\n",
        "        sym_embedded = self.sym_embedding(sym)\n",
        "        #batch_size x seq_len x embedding_dim\n",
        "        sym_embedded =  sym_embedded.transpose(1,2)\n",
        "        #batch_size x embedding_dim x seq_len\n",
        "        feature_map_bigrams = self.relu(self.bigrams(sym_embedded))\n",
        "        #batch_size x filter_count2 x seq_len* \n",
        "        feature_map_trigrams = self.relu(self.trigrams(sym_embedded))\n",
        "\n",
        "        embedded = self.word_embedding(word)   # переводим последовательность индексов в последовательность эмбеддингов\n",
        "        mean_emb = torch.mean(embedded, dim=1) # считаем средний эмбеддинг предложения\n",
        "        \n",
        "        pooling1 = feature_map_bigrams.max(2)[0] \n",
        "        pooling2 = feature_map_trigrams.max(2)[0]\n",
        "        concat = torch.cat((pooling1, pooling2), 1)\n",
        "        sym_logits = nn.Linear(concat, in_features=180, out_features=100)\n",
        " \n",
        "        concat = torch.cat((sym_logits, mean_emb), 1)\n",
        "\n",
        "        logits = self.out(self.hidden(concat))\n",
        "        return logits"
      ],
      "metadata": {
        "id": "H-eg2BdwfoeI"
      },
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data, val_data = train_test_split(corpus[['tokens', 'text', 'ttype']], test_size=0.2, stratify=corpus['ttype'], random_state=42)\n",
        "\n",
        "train_dataset = CombineDataset(train_data, word2id, symbol2id, DEVICE)\n",
        "train_sampler = RandomSampler(train_dataset)\n",
        "train_iterator = DataLoader(train_dataset, collate_fn=train_dataset.collate_fn, sampler=train_sampler, batch_size=1024)\n",
        "\n",
        "val_dataset = CombineDataset(val_data, word2id, symbol2id, DEVICE)\n",
        "val_sampler = SequentialSampler(val_dataset)\n",
        "val_iterator = DataLoader(val_dataset, collate_fn=train_dataset.collate_fn, sampler=val_sampler, batch_size=1024)"
      ],
      "metadata": {
        "id": "SxXczqgzqH_E"
      },
      "execution_count": 173,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, iterator, optimizer, criterion):\n",
        "    print('Training...')\n",
        "    epoch_loss = 0 # для подсчета среднего лосса на всех батчах\n",
        "    model.train()  # ставим модель в обучение, явно указываем, что сейчас надо будет хранить градиенты у всех весов\n",
        "\n",
        "    epoch_loss = 0\n",
        "    epoch_metric = 0\n",
        "\n",
        "    for i, (texts, symbols, ys) in enumerate(iterator): #итерируемся по батчам\n",
        "        print(i)\n",
        "        optimizer.zero_grad()  #обнуляем градиенты\n",
        "        preds_proba = model(texts, symbols) #прогоняем данные через модель\n",
        "        loss = criterion(preds_proba, ys) #считаем значение функции потерь  \n",
        "        loss.backward() #считаем градиенты  \n",
        "        optimizer.step() #обновляем веса \n",
        "        epoch_loss += loss.item() #сохраняем значение функции потерь\n",
        "        batch_metric = f1(preds_proba.round().long(), ys.long(), ignore_index=0)\n",
        "        epoch_metric += batch_metric\n",
        "\n",
        "        if not (i + 1) % int(len(iterator)/5):\n",
        "              print(f'Val loss: {epoch_loss/i}, Val f1: {epoch_metric/i}')\n",
        "        \n",
        "    return epoch_metric / len(iterator), epoch_loss / len(iterator) # возвращаем среднее значение по всей выборке\n",
        "\n",
        "\n",
        "\n",
        "def evaluate(model, iterator, criterion):\n",
        "    print(\"\\nValidating...\")\n",
        "    epoch_loss = 0\n",
        "    epoch_metric = 0\n",
        "    model.eval() \n",
        "    with torch.no_grad():\n",
        "        for i, (texts, symbols, ys) in enumerate(iterator):   \n",
        "            predictions = model(texts, symbols)  # делаем предсказания на тесте\n",
        "            loss = criterion(predictions, ys)   # считаем значения функции ошибки для статистики  \n",
        "            epoch_loss += loss.item()\n",
        "            batch_metric = f1(predictions.round().long(), ys.long(), ignore_index=0)\n",
        "            epoch_metric += batch_metric\n",
        "            if not (i + 1) % int(len(iterator)/5):\n",
        "              print(f'Val loss: {epoch_loss/i}, Val f1: {epoch_metric/i}')\n",
        "  \n",
        "        \n",
        "    return epoch_metric / len(iterator), epoch_loss / len(iterator) # возвращаем средний лосс по батчам\n",
        "\n",
        "new_model = CombCNN(len(word2id), len(symbol2id), 100, weights)\n",
        "optimizer = optim.Adam(new_model.parameters(), lr=0.001)\n",
        "criterion = nn.BCELoss() # Binary Cross Entropy\n",
        "\n",
        "# веса модели и значения лосса храним там же, где и все остальные тензоры\n",
        "new_model = new_model.to(DEVICE)\n",
        "criterion = criterion.to(DEVICE)\n",
        "\n",
        "losses_ft = []\n",
        "losses_eval_ft = []\n",
        "\n",
        "for i in range(5):\n",
        "    print(f'\\nstarting Epoch {i}')\n",
        "    epoch_loss = train(model, train_iterator, optimizer, criterion)\n",
        "    losses_ft.append(epoch_loss)\n",
        "\n",
        "    epoch_loss_on_test = evaluate(model, val_iterator, criterion)\n",
        "    losses_eval_ft.append(epoch_loss_on_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "W_UvUf1bqVXc",
        "outputId": "c2046fd8-097e-41e3-dcf6-bfbc2ec683e0"
      },
      "execution_count": 174,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "starting Epoch 0\n",
            "Training...\n",
            "0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-174-5d537f3b457f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'\\nstarting Epoch {i}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m     \u001b[0mepoch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m     \u001b[0mlosses_ft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-174-5d537f3b457f>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, iterator, optimizer, criterion)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m#обнуляем градиенты\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mpreds_proba\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msymbols\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#прогоняем данные через модель\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds_proba\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mys\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#считаем значение функции потерь\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#считаем градиенты\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-149-7004c9dc85a6>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, word, sym)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0msym_embedded\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0msym_embedded\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;31m#batch_size x embedding_dim x seq_len\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mfeature_map_bigrams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbigrams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msym_embedded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0;31m#batch_size x filter_count2 x seq_len*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mfeature_map_trigrams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrigrams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msym_embedded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 301\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    296\u001b[0m                             _single(0), self.dilation, self.groups)\n\u001b[1;32m    297\u001b[0m         return F.conv1d(input, weight, bias, self.stride,\n\u001b[0;32m--> 298\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Calculated padded input size per channel: (1). Kernel size: (2). Kernel size can't be greater than actual input size"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "запуталась("
      ],
      "metadata": {
        "id": "iWihHyYe1i5F"
      }
    }
  ]
}